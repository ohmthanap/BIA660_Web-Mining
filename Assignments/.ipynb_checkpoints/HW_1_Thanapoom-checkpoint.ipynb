{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW #1: Analyze Documents by Numpy</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanapoom Phatthanaphan <br> CWID: 20011296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: \n",
    "- Please read the problem description carefully\n",
    "- Make sure to complete all requirements (shown as bullets) . In general, it would be much easier if you complete the requirements in the order as shown in the problem description\n",
    "- Follow the Submission Instruction to submit your assignment.\n",
    "- Code of academic integrity:\n",
    "    - **Each assignment needs to be completed independently. This is NOT group assignment**. \n",
    "    - Never ever copy others' work (even with minor modification, e.g. changing variable names)\n",
    "    - If you generate code using large lanaguage models (although it is not encouraged), make sure to adapt the generated code to meet all requirements and it is executable.\n",
    "    - Anti-Plagiarism software will be used to check similarities between all submissions.\n",
    "    - Check Syllabus for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Description**\n",
    "\n",
    "In this assignment, you'll write functions to analyze an article to find out the word distributions and key concepts. \n",
    "\n",
    "The packages you'll need for this assignment include `numpy` and `string`. Some useful functions:\n",
    "- string, list, dictionary: `split`,`join`, `count`, `index`,`strip`\n",
    "- numpy: `sum`, `where`,`log`, `argsort`,`argmin`, `argmax` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze word counts in a document\n",
    "\n",
    "\n",
    "Define a function named `tokenize(doc)` which process an input document (denoted as `doc`) as follows: \n",
    "\n",
    "* First convert the document to lower case.\n",
    "* Split the document into a list of tokens by **space** (including tabs and new lines). For example, `Hello, it's a helloooo world!` -> `[\"Hello,\", \"it's\", \"a\", \"helloooo\", \"world!\"]` \n",
    "* Remove leading or trailing punctuations of each token. For example, `world!` ->`world`, but `it's` is not changed as the punctiation is in the middle. \n",
    "    - Hint, you can import module *string*, use `string.punctuation` to get a list of punctuations (say `puncts`), and then use function `strip(puncts)` to remove leading or trailing punctuations in each token\n",
    "* Find the count of each unique `non-empty` token and save the count as a dictionary, named `vocab`, i.e., `{\"Hello,\": 1, a: 1, ...}` \n",
    "* Return the dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "# add your input statement\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    \n",
    "    vocab = {}\n",
    "    \n",
    "    # Convert the text to lower case\n",
    "    lowercased_doc = doc.lower()\n",
    "    \n",
    "    # Get a string containing all punctuation characters\n",
    "    puncts = string.punctuation\n",
    "    \n",
    "    # Split the document into tokens (words)\n",
    "    tokens = lowercased_doc.split()\n",
    "    \n",
    "    # Remove leading and trailing punctuations from each token\n",
    "    cleaned_tokens = [token.strip(puncts) for token in tokens]\n",
    "    \n",
    "    # Count each token in the document\n",
    "    for token in cleaned_tokens:\n",
    "        if token:\n",
    "            vocab[token] = vocab.get(token, 0) + 1\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 1, \"it's\": 1, 'a': 1, 'helloooo': 1, 'world': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test function\n",
    "\n",
    "doc = \"Hello , it's a helloooo world!\"\n",
    "vocab = tokenize(doc)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Split unusual words into common pieces \n",
    "\n",
    "\n",
    "Notice that some words contains extra characters or punctuations. Next we'll find the common subwords in each word (e.g., split \"helloooo\" to \"hello\" and \"ooo\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1.** Define a function `get_pair_count(vocab)` to count the freqency of two subwords in a word as follows:\n",
    "\n",
    "\n",
    "- The input is a dictionary (denoted as `vocab`) which maps each word into its count. The word contains subwords delimited by space. For example, at the beginning, we treat each character as a subword. Thus, the `vocab` from Q1 is `{\"h e l l o\":1, \"a\":1, ...}`\n",
    "- Count any pair of consecutive subwords in each word and create a new dictionary to note down the total count of each pair across all the words, e.g. `{\"e l\": 2}`.\n",
    "- Return the dictionary for the subword pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_count(vocab):\n",
    "    \n",
    "    pairs = {}\n",
    "    \n",
    "    # Iterate through each word in the vocab\n",
    "    for word, count in vocab.items():\n",
    "        if len(word) > 1:\n",
    "            chars = word.split()\n",
    "\n",
    "            # Get a pair of the chars\n",
    "            for i in range(len(chars) - 1):\n",
    "                pair = (chars[i], chars[i + 1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + 1\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'h e l l o': 1, \"i t ' s\": 1, 'a': 1, 'h e l l o o o o': 1, 'w o r l d': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{('h', 'e'): 2,\n",
       " ('e', 'l'): 2,\n",
       " ('l', 'l'): 2,\n",
       " ('l', 'o'): 2,\n",
       " ('i', 't'): 1,\n",
       " ('t', \"'\"): 1,\n",
       " (\"'\", 's'): 1,\n",
       " ('o', 'o'): 3,\n",
       " ('w', 'o'): 1,\n",
       " ('o', 'r'): 1,\n",
       " ('r', 'l'): 1,\n",
       " ('l', 'd'): 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "# At the start, treat each character as a subword. \n",
    "# Add spaces as delimiters of subwords in each word \n",
    "\n",
    "init_vocab = {' '.join(list(word)) : count for word, count in vocab.items()}\n",
    "init_vocab\n",
    "\n",
    "pairs = get_pair_count(init_vocab)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2**. Define a function `merge_subwords(pair, vocab)` as follows:\n",
    "\n",
    "\n",
    "- The inputs include a subword pair (denoted as `pair`), and the original vocabulary dictionary (denoted as `vocab`).\n",
    "- For each word in `vocab`, if it contains `pair`, remove the space delimiter between the pair. Now this pair becomes a new subword. \n",
    "    - Hint: if you know regular expression, feel free to use it here. Otherwise, you can simply use function `replace`. Don't worry about some minor cross-boundary issues, e.g., `('hell' 'o')` may be matched with `hell oo`.\n",
    "- Return the new vocabuary dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords(pair, vocab):\n",
    "    \n",
    "    # initialize output vocab\n",
    "    vcab_out = {}\n",
    "    \n",
    "    # Replace a space between each pair\n",
    "    for word, count in vocab.items():\n",
    "        new_word = word.replace(f'{pair[0]} {pair[1]}', f'{pair[0]}{pair[1]}')\n",
    "        vcab_out[new_word] = count\n",
    "        \n",
    "    return vcab_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he l l o': 1, \"i t ' s\": 1, 'a': 1, 'he l l o o o o': 1, 'w o r l d': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "pair = ('h', 'e')\n",
    "\n",
    "# replace all 'h e' substrings by 'he'\n",
    "new_vocab = merge_subwords(pair, init_vocab)\n",
    "\n",
    "new_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3**. Define a function `subword_tokenize(doc, num_merges = 5)` to put all functions together.\n",
    "\n",
    "\n",
    "- The inputs include a document (denoted as `doc`) and the number of times to merge subwords.\n",
    "- Call `tokenize(doc)` to get the initial vocabulary dictionary, denoted as `vocab`\n",
    "- For each word in `vocab`, add a space delimiter between characters to indict that each character is treated as a subword initially. Save these charaters into a list named `subwords`\n",
    "- Repeat the follow steps for `num_merges` times:\n",
    "    - Call `get_pair_count(vocab)` to get the frequency of subword pair across the words\n",
    "    - Find the subword pair with the highest count, denoted as `pair`. If there is a tie, take any pair.\n",
    "    - Call `merge_subwords(pair, vocab)` to merge the selected subwords and update the vocabulary `vocab`. Add the new subword into the list `subwords`.\n",
    "- Finally, split each word in `vocab` by space to generate a new dictionary for the count of each subword.\n",
    "- Return the subword dictionary and also `subwords` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_tokenize(doc, num_merges = 5):\n",
    "    \n",
    "    vocab_out = {}\n",
    "    subwords = []\n",
    "    \n",
    "    # Get the initial vocabulary dictionary\n",
    "    vocab = tokenize(doc)\n",
    "    \n",
    "    # Add a space delimiter between characters\n",
    "    for word, count in vocab.items():\n",
    "        vocab_out[' '.join(list(word))] = count\n",
    "    \n",
    "    for merge_count in range(num_merges):\n",
    "        \n",
    "        # Treat each character as a subword initially\n",
    "        for word in vocab_out.keys():\n",
    "            for char in word.split():\n",
    "                if char not in subwords:\n",
    "                    subwords.append(char)\n",
    "\n",
    "        # Get the frequency of subword pair across the words\n",
    "        pairs = get_pair_count(vocab_out)\n",
    "\n",
    "        # Descending sort to get the highest count of the subword pair\n",
    "        sorted_pairs = sorted(pairs.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        # Merge the selected subwords and update the vocabulary\n",
    "        vocab_out = merge_subwords(sorted_pairs[0][0], vocab_out)\n",
    "\n",
    "        # Print the output\n",
    "        print(f'Merge: #{merge_count + 1}')\n",
    "        print(f'Pair: {sorted_pairs[0][0]}')\n",
    "        print(f'Vocab: {vocab_out}')\n",
    "        print(f'Subwords: {subwords}')\n",
    "    \n",
    "    return vocab_out, subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge: #1\n",
      "Pair: ('o', 'o')\n",
      "Vocab: {'h e l l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'h e l l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a']\n",
      "Merge: #2\n",
      "Pair: ('h', 'e')\n",
      "Vocab: {'he l l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'he l l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo']\n",
      "Merge: #3\n",
      "Pair: ('he', 'l')\n",
      "Vocab: {'hel l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'hel l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he']\n",
      "Merge: #4\n",
      "Pair: ('hel', 'l')\n",
      "Vocab: {'hell o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel']\n",
      "Merge: #5\n",
      "Pair: ('hell', 'o')\n",
      "Vocab: {'hello': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell']\n",
      "Merge: #6\n",
      "Pair: ('w', 'o')\n",
      "Vocab: {'hello': 1, 'wo r l d': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo']\n",
      "Merge: #7\n",
      "Pair: ('wo', 'r')\n",
      "Vocab: {'hello': 1, 'wor l d': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo', 'wo']\n",
      "Merge: #8\n",
      "Pair: ('wor', 'l')\n",
      "Vocab: {'hello': 1, 'worl d': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo', 'wo', 'wor']\n",
      "Merge: #9\n",
      "Pair: ('worl', 'd')\n",
      "Vocab: {'hello': 1, 'world': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo', 'wo', 'wor', 'worl']\n",
      "vocab:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hello': 1, 'world': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subwords:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'e',\n",
       " 'l',\n",
       " 'o',\n",
       " 'w',\n",
       " 'r',\n",
       " 'd',\n",
       " 'i',\n",
       " 't',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'oo',\n",
       " 'he',\n",
       " 'hel',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'helloo',\n",
       " 'wo',\n",
       " 'wor',\n",
       " 'worl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "# for debugging, you can print out the result of each merge as shown below.\n",
    "\n",
    "doc = \"Hello world, it's a helloooo world!\"\n",
    "vocab_out, subwords = subword_tokenize(doc, num_merges = 9)\n",
    "\n",
    "print(\"vocab:\")\n",
    "vocab_out\n",
    "\n",
    "print(\"subwords:\")\n",
    "subwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Generate a document term matrix (DTM) as a numpy array\n",
    "\n",
    "\n",
    "Define a function `get_dtm(docs)` as follows:\n",
    "- The input is a list of documents, denoted as `docs`\n",
    "- For each document, call `tokenize(doc)` defined in **Q1** (let's only use the simple version for now) to get the vocabulary dictionary \n",
    "- Pool the keys from all the dictionaries to get a list of unique words, denoted as `unique_words` \n",
    "- Creates a numpy array (denoted as `dtm`) with a shape of (# of documents x # of unique words), and set the initial values to 0. \n",
    "- Fill cell `dtm[i,j]` with the count of the `j`th word in the `i`th document \n",
    "- Return `dtm` and `unique_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtm(docs):\n",
    "    \n",
    "    # get all words\n",
    "    unique_words = []\n",
    "    words_loc = {}\n",
    "    words_each_doc = []\n",
    "    for doc in docs:\n",
    "        temp_list_words = []\n",
    "        vocab = tokenize(doc)\n",
    "        for word, count in vocab.items():\n",
    "            temp_list_words.append(word)\n",
    "            if word not in unique_words:\n",
    "                unique_words.append(word)\n",
    "                words_loc[word] = len(unique_words) - 1\n",
    "        words_each_doc.append(temp_list_words)\n",
    "    \n",
    "    # Create a numpy array\n",
    "    dtm = np.zeros((len(docs), len(unique_words)))\n",
    "    \n",
    "    # Fill the cell with the count of the word in the document\n",
    "    for i in range(len(docs)):\n",
    "        for j, word in enumerate(unique_words):\n",
    "            if word in words_each_doc[i]:\n",
    "                dtm[i, j] += 1\n",
    "            \n",
    "    return dtm, unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['hello', \"it's\", 'a', 'helloooo', 'world', 'again', 'it', 'is']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"Hello , it's a helloooo world!\",\n",
    "       \"Again, it is hello world!\"]\n",
    "\n",
    "dtm, words = get_dtm(docs)\n",
    "dtm\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test document collection). This document can be found at https://hbr.org/2022/04/the-power-of-natural-language-processing\n",
    "\n",
    "# treat each paragraph as a document\n",
    "\n",
    "docs = open(\"chatgpt.txt\", 'r').readlines()\n",
    "\n",
    "dtm, words = get_dtm(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 314)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"Ethan Mollick has a message for the humans and the machines: can't we all just get along?\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['ethan',\n",
       " 'mollick',\n",
       " 'has',\n",
       " 'a',\n",
       " 'message',\n",
       " 'for',\n",
       " 'the',\n",
       " 'humans',\n",
       " 'and',\n",
       " 'machines',\n",
       " \"can't\",\n",
       " 'we',\n",
       " 'all',\n",
       " 'just',\n",
       " 'get',\n",
       " 'along']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['22-year-old',\n",
       " 'a',\n",
       " 'a.i',\n",
       " 'about',\n",
       " 'abroad',\n",
       " 'academic',\n",
       " 'access',\n",
       " 'acknowledge',\n",
       " 'adapt',\n",
       " 'admits',\n",
       " 'adopted',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'agrees',\n",
       " 'all',\n",
       " 'allowing',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'already',\n",
       " 'alternates',\n",
       " 'an',\n",
       " 'and',\n",
       " 'anxiety',\n",
       " 'any',\n",
       " 'app',\n",
       " 'are',\n",
       " 'artificial',\n",
       " 'as',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assessments',\n",
       " 'associate',\n",
       " 'at',\n",
       " 'away',\n",
       " 'b',\n",
       " 'b-minus',\n",
       " 'banned',\n",
       " 'be',\n",
       " 'been',\n",
       " 'before',\n",
       " 'behalf',\n",
       " 'believes',\n",
       " 'between',\n",
       " 'bot',\n",
       " \"bot's\",\n",
       " 'but',\n",
       " 'by',\n",
       " 'calculators',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'capability',\n",
       " 'challenge',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changes',\n",
       " 'chatbot',\n",
       " 'chatgpt',\n",
       " 'cheating',\n",
       " 'check',\n",
       " 'cites',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'classroom',\n",
       " 'code',\n",
       " 'come',\n",
       " 'company',\n",
       " 'compose',\n",
       " 'computer',\n",
       " 'concerns',\n",
       " 'convinced',\n",
       " 'core',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " 'crashed',\n",
       " 'created',\n",
       " 'deserve',\n",
       " 'despite',\n",
       " 'detect',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'differently',\n",
       " 'districts',\n",
       " 'do',\n",
       " \"don't\",\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'educators',\n",
       " 'edward',\n",
       " 'emerging',\n",
       " 'enthusiasm',\n",
       " 'entrepreneurship',\n",
       " 'errors',\n",
       " 'essays',\n",
       " 'estimates',\n",
       " 'ethan',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'exam',\n",
       " 'exams',\n",
       " 'experimenting',\n",
       " 'facilitate',\n",
       " 'failure',\n",
       " 'far',\n",
       " 'fed',\n",
       " 'final',\n",
       " 'first',\n",
       " 'for',\n",
       " 'formally',\n",
       " 'found',\n",
       " 'from',\n",
       " 'further',\n",
       " 'generate',\n",
       " 'get',\n",
       " 'given',\n",
       " 'going',\n",
       " 'good',\n",
       " 'gptzero',\n",
       " 'grander',\n",
       " 'great',\n",
       " 'guardrails',\n",
       " 'had',\n",
       " 'happening',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'his',\n",
       " 'honest',\n",
       " 'honesty',\n",
       " 'how',\n",
       " 'human',\n",
       " 'humans',\n",
       " 'i',\n",
       " \"i'm\",\n",
       " 'ideas',\n",
       " 'if',\n",
       " 'importantly',\n",
       " 'in',\n",
       " 'incorrect',\n",
       " 'indications',\n",
       " 'information',\n",
       " 'innovation',\n",
       " 'intelligence',\n",
       " 'interactions',\n",
       " 'interrogated',\n",
       " 'into',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'just',\n",
       " 'kenya',\n",
       " 'know',\n",
       " 'launched',\n",
       " 'like',\n",
       " 'lot',\n",
       " 'machine',\n",
       " 'machines',\n",
       " 'many',\n",
       " 'math',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'mba',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'message',\n",
       " 'misleading',\n",
       " 'mollick',\n",
       " \"mollick's\",\n",
       " 'month',\n",
       " 'most',\n",
       " 'motivation',\n",
       " 'move',\n",
       " 'named',\n",
       " 'nature',\n",
       " 'need',\n",
       " 'new',\n",
       " 'not',\n",
       " 'november',\n",
       " 'now',\n",
       " 'npr',\n",
       " 'occasionally',\n",
       " 'of',\n",
       " 'officially',\n",
       " 'omissions',\n",
       " 'on',\n",
       " 'one',\n",
       " 'only',\n",
       " 'openai',\n",
       " 'or',\n",
       " 'other',\n",
       " 'overuse',\n",
       " 'partially',\n",
       " 'pass',\n",
       " \"pennsylvania's\",\n",
       " 'people',\n",
       " 'perhaps',\n",
       " 'place',\n",
       " 'places',\n",
       " 'poetry',\n",
       " 'points',\n",
       " 'policies',\n",
       " 'policy',\n",
       " 'popular',\n",
       " 'post-chatgpt',\n",
       " 'prestigious',\n",
       " 'princeton',\n",
       " 'probably',\n",
       " 'problems',\n",
       " 'professor',\n",
       " 'project',\n",
       " 'projects',\n",
       " 'prompts',\n",
       " 'provided',\n",
       " 'put',\n",
       " 'questions',\n",
       " 'raised',\n",
       " 'ran',\n",
       " 'readily',\n",
       " 'reads',\n",
       " 'reason',\n",
       " 'reasons',\n",
       " 'recently',\n",
       " 'require',\n",
       " 'required',\n",
       " 'responsible',\n",
       " 'result',\n",
       " 'results',\n",
       " 'right',\n",
       " 'running',\n",
       " 'said',\n",
       " 'scale',\n",
       " 'school',\n",
       " 'session',\n",
       " 'set',\n",
       " 'share',\n",
       " 'should',\n",
       " 'shying',\n",
       " 'since',\n",
       " 'skill',\n",
       " 'so',\n",
       " 'solve',\n",
       " 'some',\n",
       " 'something',\n",
       " 'sources',\n",
       " 'stanford',\n",
       " 'states',\n",
       " 'stop',\n",
       " 'stopped',\n",
       " 'student',\n",
       " 'students',\n",
       " 'stuff',\n",
       " 'sudden',\n",
       " 'surprising',\n",
       " 'survey',\n",
       " 'syllabus',\n",
       " 'taught',\n",
       " 'teach',\n",
       " 'teaches',\n",
       " 'tell',\n",
       " 'testing',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'then',\n",
       " 'there',\n",
       " 'they',\n",
       " 'think',\n",
       " 'this',\n",
       " 'thousands',\n",
       " 'tian',\n",
       " 'time',\n",
       " 'times',\n",
       " 'to',\n",
       " 'told',\n",
       " 'tool',\n",
       " 'truly',\n",
       " 'truth',\n",
       " 'try',\n",
       " 'university',\n",
       " 'up',\n",
       " 'use',\n",
       " 'used',\n",
       " 'users',\n",
       " 'using',\n",
       " 'violation',\n",
       " 'want',\n",
       " 'warned',\n",
       " 'was',\n",
       " 'we',\n",
       " \"we're\",\n",
       " 'week',\n",
       " 'were',\n",
       " 'wharton',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'will',\n",
       " 'with',\n",
       " 'without',\n",
       " 'world',\n",
       " 'would',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'written',\n",
       " 'wrong',\n",
       " 'year',\n",
       " 'yet']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.shape\n",
    "\n",
    "# check words in a paragraph\n",
    "p = 0 # paragraph id\n",
    "docs[p]\n",
    "[w for i,w in enumerate(words) if dtm[p][i]>0]\n",
    "\n",
    "sorted(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 Analyze DTM Array (4 points)\n",
    "\n",
    "\n",
    "**Don't use any loop in this task**. You should use array operations to take the advantage of high performance computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named `analyze_dtm(dtm, words, docs)` as follows:\n",
    "- It takes an array `dtm`, an array of `words`, and an array of documents (denoted `docs`) as inputs, where `dtm` is the array you created from `docs` in Q3 with a shape of $(m \\times n)$, and `words` corresponds to the columns of `dtm`.\n",
    "- Calculate the document frequency for each word $j$, e.g., how many documents contain word $j$. Save the result to array $df$. $df$ has shape of $(n,)$ or $(1, n)$. \n",
    "- Normalize the word count per paragraph: divides word count, i.e., $dtm_{i,j}$, by the total number of words in document $i$. Save the result as an array named $tf$. $tf$ has shape of $(m,n)$. \n",
    "* For each $dtm_{i,j}$, calculate $tfidf_{i,j} = \\frac{tf_{i, j}}{1+log(df_j)}$, i.e., divide each normalized word count by the log of the document frequency of the word (add 1 to the denominator to avoid dividing by 0).  $tfidf$ has shape of $(m,n)$ \n",
    "* Print out the following:\n",
    "    \n",
    "    - the total number of words in the documents represented by `dtm` \n",
    "    - the number of documents and the number of unique words\n",
    "    - the most frequent top 10 words in this document    \n",
    "    - top-5 words that show in most of the documents, i.e. words with the top 5 largest $df$ values (print words first, then their values. ) \n",
    "    - the longest document in terms of the number of words. Print out this document.\n",
    "    - top-5 words with the largest $tfidf$ values in the longest document (show words and values) \n",
    "    - documents that contain `intelligence` word.\n",
    "\n",
    "Note, for all the steps, **do not use any loop**. Just use array functions and broadcasting for high performance computation.\n",
    "\n",
    "Your answer may be different from the example output, since words may have the same values in the dtm but are kept in different positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dtm(dtm, words, docs):\n",
    "    \n",
    "    # Calculate the document frequency for each word\n",
    "    df = np.sum(dtm>0, axis=0)\n",
    "    \n",
    "    # Normalize the word count per paragraph: divides word count by the total number of words in document\n",
    "    tf = dtm / np.sum(dtm, axis=1, keepdims=True)\n",
    "    \n",
    "    # Divide each normalized word count by the log of the document frequency of the word\n",
    "    tfidf = tf / (1 + np.log(df))\n",
    "    \n",
    "    # The total number of words in the documents represented by dtm\n",
    "    total_words = np.sum(dtm)\n",
    "    print(\"The total number of words in the documents represented by dtm:\", total_words)\n",
    "    \n",
    "    # The number of documents and the number of unique words\n",
    "    num_docs, num_unique_words = dtm.shape\n",
    "    print(\"\\nThe number of documents:\", num_docs)\n",
    "    print(\"\\nThe number of unique words:\", num_unique_words)\n",
    "    \n",
    "    # The most frequent top 10 words in this document\n",
    "    ind_words_dtm = np.argsort(np.sum(dtm, axis=0))[::-1]\n",
    "    top10_words = [words[i] for i in ind_words_dtm[:10]]\n",
    "    print(\"\\nThe most frequent top 10 words in this document:\", top10_words)\n",
    "    \n",
    "    # The top-5 words that show in most of the documents\n",
    "    ind_words_df = np.argsort(df)[::-1]\n",
    "    print(\"\\nThe top-5 words that show in most of the documents:\")\n",
    "    for i in ind_words_df[:5]:\n",
    "        print(f\"{words[i]}: {df[i]}\")\n",
    "    \n",
    "    # The longest document in terms of the number of words\n",
    "    num_words_each_doc = np.argsort(np.sum(dtm, axis=1))[::-1]\n",
    "    print(\"\\nThe longest document in terms of the number of words:\\n\", docs[num_words_each_doc[0]])\n",
    "    \n",
    "    # The top-5 words with the largest tfidf values in the longest document (show words and values)\n",
    "    ind_words_tfidf = np.argsort(tfidf[num_words_each_doc[0]])[::-1]\n",
    "    print(\"\\nThe top-5 words with the largest tfidf values in the longest document:\")\n",
    "    for i in ind_words_tfidf[:5]:\n",
    "        print(f\"{words[i]}: {tfidf[num_words_each_doc[0]][i]}\")\n",
    "    \n",
    "    # Documents that contain \"intelligence\" word\n",
    "    docs_contain_intelligence = np.where(dtm[:, np.where(words == 'intelligence')[0]] > 0)\n",
    "    print(\"\\nDocuments that contain 'intelligence' word:\")\n",
    "    print(\"Document No.:\", docs_contain_intelligence[0][0])\n",
    "    print(docs[docs_contain_intelligence[0][0]])\n",
    "    print(\"Document No.:\", docs_contain_intelligence[0][1])\n",
    "    print(docs[docs_contain_intelligence[0][1]])\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words in the documents represented by dtm: 624.0\n",
      "\n",
      "The number of documents: 26\n",
      "\n",
      "The number of unique words: 314\n",
      "\n",
      "The most frequent top 10 words in this document: ['the', 'and', 'to', 'a', 'in', 'it', 'that', 'he', 'chatgpt', 'for']\n",
      "\n",
      "The top-5 words that show in most of the documents:\n",
      "the: 20\n",
      "and: 15\n",
      "to: 14\n",
      "a: 12\n",
      "in: 11\n",
      "\n",
      "The longest document in terms of the number of words:\n",
      " \"\"\"I think everybody is cheating ... I mean, it's happening. So what I'm asking students to do is just be honest with me,\"\" he said. \"\"Tell me what they use ChatGPT for, tell me what they used as prompts to get it to do what they want, and that's all I'm asking from them. We're in a world where this is happening, but now it's just going to be at an even grander scale.\"\"\"\n",
      "\n",
      "\n",
      "The top-5 words with the largest tfidf values in the longest document:\n",
      "what: 0.019230769230769232\n",
      "me: 0.019230769230769232\n",
      "everybody: 0.019230769230769232\n",
      "mean: 0.019230769230769232\n",
      "it's: 0.019230769230769232\n",
      "\n",
      "Documents that contain 'intelligence' word:\n",
      "Document No.: 4\n",
      "\"Some school districts have banned access to the bot, and not without reason. The artificial intelligence tool from the company OpenAI can compose poetry. It can write computer code. It can maybe even pass an MBA exam.\"\n",
      "\n",
      "Document No.: 14\n",
      "\"He readily admits he alternates between enthusiasm and anxiety about how artificial intelligence can change assessments in the classroom, but he believes educators need to move with the times.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = np.array(words)\n",
    "docs = np.array(docs)\n",
    "\n",
    "analyze_dtm(dtm, words, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 (Bonus). Generating DTM by subword tokenization (2 points)\n",
    "\n",
    "Assume you only need to keep the top N most frequent words (e.g., N = 200) in the collection of documents. Redo Q3-Q4 as follows:\n",
    "\n",
    "- Use the subword tokenization you developed in Q2 to tokenize documents\n",
    "- Generate a dtm with only the top-N most frequent words in the entire collection.\n",
    "- Then analyze the dtm as in Q4.\n",
    "\n",
    "\n",
    "Describe and implement your ideas. Again, no loop should be used in your solution to Q4. **Don't just submit code. You need to explain your idea as markdowns. No score will be given if only code is submitted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Q5:\n",
    "# To generate a dtm with only the top-N most frequent words in the entire collection.\n",
    "# We can still use the same function in Q4 with some changes because, in that function,\n",
    "# We createda list of words with descending orders by the frequency of each word.\n",
    "# Therefore, to get only the top-N, we can easily additionally specify N number in a code\n",
    "# For example, I will create a copy of Q4 function with the changing points for Q5\n",
    "# Please look the test code below\n",
    "    \n",
    "def analyze_dtm_forQ5(dtm, words, docs, N):\n",
    "    \n",
    "    # Calculate the document frequency for each word\n",
    "    df = np.sum(dtm>0, axis=0)\n",
    "    \n",
    "    # Normalize the word count per paragraph: divides word count by the total number of words in document\n",
    "    tf = dtm / np.sum(dtm, axis=1, keepdims=True)\n",
    "    \n",
    "    # Divide each normalized word count by the log of the document frequency of the word\n",
    "    tfidf = tf / (1 + np.log(df))\n",
    "    \n",
    "    # The total number of words in the documents represented by dtm\n",
    "    total_words = np.sum(dtm)\n",
    "    print(\"The total number of words in the documents represented by dtm:\", total_words)\n",
    "    \n",
    "    # The number of documents and the number of unique words\n",
    "    num_docs, num_unique_words = dtm.shape\n",
    "    print(\"\\nThe number of documents:\", num_docs)\n",
    "    print(\"\\nThe number of unique words:\", num_unique_words)\n",
    "    \n",
    "    # The most frequent top N words in this document\n",
    "    # ** Change from top 10 to top N words by specifying N number in a code **\n",
    "    ind_words_dtm = np.argsort(np.sum(dtm, axis=0))[::-1]\n",
    "    topN_words = [words[i] for i in ind_words_dtm[:N]]\n",
    "    print(\"\\nThe most frequent top N words in this document:\", topN_words)\n",
    "    \n",
    "    # The top-N words that show in most of the documents\n",
    "    # ** Change from top 10 to top N words by specifying N number in a code **\n",
    "    ind_words_df = np.argsort(df)[::-1]\n",
    "    print(\"\\nThe top-N words that show in most of the documents:\")\n",
    "    for i in ind_words_df[:N]:\n",
    "        print(f\"{words[i]}: {df[i]}\")\n",
    "    \n",
    "    # The longest document in terms of the number of words\n",
    "    num_words_each_doc = np.argsort(np.sum(dtm, axis=1))[::-1]\n",
    "    print(\"\\nThe longest document in terms of the number of words:\\n\", docs[num_words_each_doc[0]])\n",
    "    \n",
    "    # The top-N words with the largest tfidf values in the longest document (show words and values)\n",
    "    # ** Change from top 10 to top N words by specifying N number in a code **\n",
    "    ind_words_tfidf = np.argsort(tfidf[num_words_each_doc[0]])[::-1]\n",
    "    print(\"\\nThe top-N words with the largest tfidf values in the longest document:\")\n",
    "    for i in ind_words_tfidf[:N]:\n",
    "        print(f\"{words[i]}: {tfidf[num_words_each_doc[0]][i]}\")\n",
    "    \n",
    "    # Documents that contain \"intelligence\" word\n",
    "    docs_contain_intelligence = np.where(dtm[:, np.where(words == 'intelligence')[0]] > 0)\n",
    "    print(\"\\nDocuments that contain 'intelligence' word:\")\n",
    "    print(\"Document No.:\", docs_contain_intelligence[0][0])\n",
    "    print(docs[docs_contain_intelligence[0][0]])\n",
    "    print(\"Document No.:\", docs_contain_intelligence[0][1])\n",
    "    print(docs[docs_contain_intelligence[0][1]])\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put everything together and test using main block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======Q1: =========\n",
      "\n",
      "{'hello': 1, \"it's\": 1, 'a': 1, 'helloooo': 1, 'world': 1}\n",
      "\n",
      "=======Q2: =========\n",
      "\n",
      "Merge: #1\n",
      "Pair: ('o', 'o')\n",
      "Vocab: {'h e l l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'h e l l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a']\n",
      "Merge: #2\n",
      "Pair: ('h', 'e')\n",
      "Vocab: {'he l l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'he l l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo']\n",
      "Merge: #3\n",
      "Pair: ('he', 'l')\n",
      "Vocab: {'hel l o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'hel l oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he']\n",
      "Merge: #4\n",
      "Pair: ('hel', 'l')\n",
      "Vocab: {'hell o': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'hell oo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel']\n",
      "Merge: #5\n",
      "Pair: ('hell', 'o')\n",
      "Vocab: {'hello': 1, 'w o r l d': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell']\n",
      "Merge: #6\n",
      "Pair: ('w', 'o')\n",
      "Vocab: {'hello': 1, 'wo r l d': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo']\n",
      "Merge: #7\n",
      "Pair: ('wo', 'r')\n",
      "Vocab: {'hello': 1, 'wor l d': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo', 'wo']\n",
      "Merge: #8\n",
      "Pair: ('wor', 'l')\n",
      "Vocab: {'hello': 1, 'worl d': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo', 'wo', 'wor']\n",
      "Merge: #9\n",
      "Pair: ('worl', 'd')\n",
      "Vocab: {'hello': 1, 'world': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "Subwords: ['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo', 'wo', 'wor', 'worl']\n",
      "vocab:\n",
      "{'hello': 1, 'world': 2, \"i t ' s\": 1, 'a': 1, 'helloo oo': 1}\n",
      "subwords:\n",
      "['h', 'e', 'l', 'o', 'w', 'r', 'd', 'i', 't', \"'\", 's', 'a', 'oo', 'he', 'hel', 'hell', 'hello', 'helloo', 'wo', 'wor', 'worl']\n",
      "\n",
      "=======Q3: =========\n",
      "\n",
      "[[1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1. 1. 1.]]\n",
      "['hello', \"it's\", 'a', 'helloooo', 'world', 'again', 'it', 'is']\n",
      "\n",
      "=======Q4: =========\n",
      "\n",
      "The total number of words in the documents represented by dtm: 624.0\n",
      "\n",
      "The number of documents: 26\n",
      "\n",
      "The number of unique words: 314\n",
      "\n",
      "The most frequent top 10 words in this document: ['the', 'and', 'to', 'a', 'in', 'it', 'that', 'he', 'chatgpt', 'for']\n",
      "\n",
      "The top-5 words that show in most of the documents:\n",
      "the: 20\n",
      "and: 15\n",
      "to: 14\n",
      "a: 12\n",
      "in: 11\n",
      "\n",
      "The longest document in terms of the number of words:\n",
      " \"\"\"I think everybody is cheating ... I mean, it's happening. So what I'm asking students to do is just be honest with me,\"\" he said. \"\"Tell me what they use ChatGPT for, tell me what they used as prompts to get it to do what they want, and that's all I'm asking from them. We're in a world where this is happening, but now it's just going to be at an even grander scale.\"\"\"\n",
      "\n",
      "\n",
      "The top-5 words with the largest tfidf values in the longest document:\n",
      "what: 0.019230769230769232\n",
      "me: 0.019230769230769232\n",
      "everybody: 0.019230769230769232\n",
      "mean: 0.019230769230769232\n",
      "it's: 0.019230769230769232\n",
      "\n",
      "Documents that contain 'intelligence' word:\n",
      "Document No.: 4\n",
      "\"Some school districts have banned access to the bot, and not without reason. The artificial intelligence tool from the company OpenAI can compose poetry. It can write computer code. It can maybe even pass an MBA exam.\"\n",
      "\n",
      "Document No.: 14\n",
      "\"He readily admits he alternates between enthusiasm and anxiety about how artificial intelligence can change assessments in the classroom, but he believes educators need to move with the times.\"\n",
      "\n",
      "\n",
      "=======Q5: BONUS =========\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'analyze_dtm_forQ5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bk/66r4ld3j7hj8yg_49fhv2fr40000gn/T/ipykernel_89420/302563382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# We additionally specify N number in a code to get top-N words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0manalyze_dtm_forQ5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_dtm_forQ5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyze_dtm_forQ5' is not defined"
     ]
    }
   ],
   "source": [
    "# best practice to test your class\n",
    "# if your script is exported as a module,\n",
    "# the following part is ignored\n",
    "# this is equivalent to main() in Java\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    \n",
    "    print(\"\\n=======Q1: =========\\n\")\n",
    "    doc = \"Hello , it's a helloooo world!\"\n",
    "    vocab = tokenize(doc)\n",
    "    print(vocab)\n",
    "    \n",
    "    print(\"\\n=======Q2: =========\\n\")\n",
    "    doc = \"Hello world, it's a helloooo world!\"\n",
    "\n",
    "    vocab_out, subwords = subword_tokenize(doc, num_merges = 9)\n",
    "\n",
    "    print(\"vocab:\")\n",
    "    print(vocab_out)\n",
    "\n",
    "    print(\"subwords:\")\n",
    "    print(subwords)\n",
    "    \n",
    "    print(\"\\n=======Q3: =========\\n\")\n",
    "    \n",
    "    docs = [\"Hello , it's a helloooo world!\",\n",
    "       \"Again, it is hello world!\"]\n",
    "\n",
    "    dtm, words = get_dtm(docs)\n",
    "    print(dtm)\n",
    "    print(words)\n",
    "    \n",
    "    print(\"\\n=======Q4: =========\\n\")\n",
    "    \n",
    "    docs = open(\"chatgpt.txt\", 'r').readlines()\n",
    "\n",
    "    dtm, words = get_dtm(docs)\n",
    "\n",
    "    words = np.array(words)\n",
    "    docs = np.array(docs)\n",
    "\n",
    "    analyze_dtm(dtm, words, docs)\n",
    "    \n",
    "    print(\"\\n=======Q5: BONUS =========\\n\")\n",
    "    \n",
    "    # To generate a dtm with only the top-N most frequent words in the entire collection.\n",
    "    # We can still use the same function in Q4 with some changes because, in that function,\n",
    "    # We createda list of words with descending orders by the frequency of each word.\n",
    "    # Therefore, to get only the top-N, we can easily additionally specify N number in a code\n",
    "    # For example, I will create a copy of Q4 function with the changing points for Q5\n",
    "    # Please look the test code below\n",
    "    \n",
    "def analyze_dtm_forQ5(dtm, words, docs, N):\n",
    "    \n",
    "    # Calculate the document frequency for each word\n",
    "    df = np.sum(dtm>0, axis=0)\n",
    "    \n",
    "    # Normalize the word count per paragraph: divides word count by the total number of words in document\n",
    "    tf = dtm / np.sum(dtm, axis=1, keepdims=True)\n",
    "    \n",
    "    # Divide each normalized word count by the log of the document frequency of the word\n",
    "    tfidf = tf / (1 + np.log(df))\n",
    "    \n",
    "    # The total number of words in the documents represented by dtm\n",
    "    total_words = np.sum(dtm)\n",
    "    print(\"The total number of words in the documents represented by dtm:\", total_words)\n",
    "    \n",
    "    # The number of documents and the number of unique words\n",
    "    num_docs, num_unique_words = dtm.shape\n",
    "    print(\"\\nThe number of documents:\", num_docs)\n",
    "    print(\"\\nThe number of unique words:\", num_unique_words)\n",
    "    \n",
    "    # The most frequent top N words in this document\n",
    "    # ** Change from top 10 to top N words by specifying N number in a code **\n",
    "    ind_words_dtm = np.argsort(np.sum(dtm, axis=0))[::-1]\n",
    "    topN_words = [words[i] for i in ind_words_dtm[:N]]\n",
    "    print(\"\\nThe most frequent top N words in this document:\", topN_words)\n",
    "    \n",
    "    # The top-N words that show in most of the documents\n",
    "    # ** Change from top 10 to top N words by specifying N number in a code **\n",
    "    ind_words_df = np.argsort(df)[::-1]\n",
    "    print(\"\\nThe top-N words that show in most of the documents:\")\n",
    "    for i in ind_words_df[:N]:\n",
    "        print(f\"{words[i]}: {df[i]}\")\n",
    "    \n",
    "    # The longest document in terms of the number of words\n",
    "    num_words_each_doc = np.argsort(np.sum(dtm, axis=1))[::-1]\n",
    "    print(\"\\nThe longest document in terms of the number of words:\\n\", docs[num_words_each_doc[0]])\n",
    "    \n",
    "    # The top-N words with the largest tfidf values in the longest document (show words and values)\n",
    "    # ** Change from top 10 to top N words by specifying N number in a code **\n",
    "    ind_words_tfidf = np.argsort(tfidf[num_words_each_doc[0]])[::-1]\n",
    "    print(\"\\nThe top-N words with the largest tfidf values in the longest document:\")\n",
    "    for i in ind_words_tfidf[:N]:\n",
    "        print(f\"{words[i]}: {tfidf[num_words_each_doc[0]][i]}\")\n",
    "    \n",
    "    # Documents that contain \"intelligence\" word\n",
    "    docs_contain_intelligence = np.where(dtm[:, np.where(words == 'intelligence')[0]] > 0)\n",
    "    print(\"\\nDocuments that contain 'intelligence' word:\")\n",
    "    print(\"Document No.:\", docs_contain_intelligence[0][0])\n",
    "    print(docs[docs_contain_intelligence[0][0]])\n",
    "    print(\"Document No.:\", docs_contain_intelligence[0][1])\n",
    "    print(docs[docs_contain_intelligence[0][1]])\n",
    "    \n",
    "    return None\n",
    "\n",
    "# We additionally specify N number in a code to get top-N words\n",
    "analyze_dtm_forQ5(dtm, words, docs, N=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
