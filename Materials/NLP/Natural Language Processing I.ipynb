{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (I)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NLTK installation\n",
    " 1. Install NLTK package using: pip install nltk \n",
    " 2. Open your python editor (Jupyter Notebook, Spyder etc.) and type the following comands below. Select \"all packages\" to install data included in NLTK, including corpora and books. It may take a few minutes to download all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into tokens, phrases, or segments\n",
    "   * Clean up tokens and annotate tokens\n",
    "   * Extract features from tokens for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words, phrases, or segments\n",
    "   * Remove stop words and filter tokens\n",
    "   * POS (part of speech) Tagging\n",
    "   * Normalization: Stemming, Lemmatization\n",
    "   * Named Entity Recognition (NER)\n",
    "   * Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    "   * Document-to-term matrix (bag of words)\n",
    " - NLP packages: NLTK, Gensim, spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import re    # import re module\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The FDA setting a minimum recommendation for efficacy doesn't mean vaccines couldn't perform better. The benchmark is also a reminder that COVID-19 vaccine development is in its early days. If the first vaccines made available only meet the minimum, they may be replaced by others that prove to protect more people. But with more than 1 million deaths from COVID-19 worldwide — and U.S. deaths surpassing 200,000 — the urgency in finding a vaccine that safely helps at least some people is at the forefront.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this extract is from https://www.sciencenews.org/article/coronavirus-what-does-covid-19-vaccine-efficacy-mean\n",
    "\n",
    "text = \"The FDA setting a minimum recommendation for efficacy doesn't mean vaccines \\\n",
    "couldn't perform better. The benchmark is also a reminder that COVID-19 vaccine \\\n",
    "development is in its early days. If the first vaccines made available only meet \\\n",
    "the minimum, they may be replaced by others that prove to protect more people. \\\n",
    "But with more than 1 million deaths from COVID-19 worldwide — \\\n",
    "and U.S. deaths surpassing 200,000 — the urgency in finding a \\\n",
    "vaccine that safely helps at least some people is at the forefront.\"\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    " - **Definition**: the process of breaking a stream of textual content up into words, terms, symbols, or some other meaningful elements called tokens.\n",
    "    * Word (Unigram)\n",
    "    * Bigram (Two consecutive words)\n",
    "    * Trigram (Three consecutive words)\n",
    "    * Sentence\n",
    " - Different methods exist:\n",
    "    * Split by regular expression patterns\n",
    "    * NLTK's word tokenizer\n",
    "    * NLTK's regular expression tokenizer (customizable)\n",
    " - None of them can be perfect for any tokenization task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['The', 'FDA', 'setting', 'a', 'minimum', 'recommendation', 'for', 'efficacy', 'doesn', 't', 'mean', 'vaccines', 'couldn', 't', 'perform', 'better', 'The', 'benchmark', 'is', 'also', 'a', 'reminder', 'that', 'COVID', '19', 'vaccine', 'development', 'is', 'in', 'its', 'early', 'days', 'If', 'the', 'first', 'vaccines', 'made', 'available', 'only', 'meet', 'the', 'minimum', 'they', 'may', 'be', 'replaced', 'by', 'others', 'that', 'prove', 'to', 'protect', 'more', 'people', 'But', 'with', 'more', 'than', '1', 'million', 'deaths', 'from', 'COVID', '19', 'worldwide', 'and', 'U', 'S', 'deaths', 'surpassing', '200', '000', 'the', 'urgency', 'in', 'finding', 'a', 'vaccine', 'that', 'safely', 'helps', 'at', 'least', 'some', 'people', 'is', 'at', 'the', 'forefront', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'FDA',\n",
       " 'setting',\n",
       " 'a',\n",
       " 'minimum',\n",
       " 'recommendation',\n",
       " 'for',\n",
       " 'efficacy',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'mean',\n",
       " 'vaccines',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'perform',\n",
       " 'better',\n",
       " 'The',\n",
       " 'benchmark',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'reminder',\n",
       " 'that',\n",
       " 'COVID',\n",
       " '19',\n",
       " 'vaccine',\n",
       " 'development',\n",
       " 'is',\n",
       " 'in',\n",
       " 'its',\n",
       " 'early',\n",
       " 'days',\n",
       " 'If',\n",
       " 'the',\n",
       " 'first',\n",
       " 'vaccines',\n",
       " 'made',\n",
       " 'available',\n",
       " 'only',\n",
       " 'meet',\n",
       " 'the',\n",
       " 'minimum',\n",
       " 'they',\n",
       " 'may',\n",
       " 'be',\n",
       " 'replaced',\n",
       " 'by',\n",
       " 'others',\n",
       " 'that',\n",
       " 'prove',\n",
       " 'to',\n",
       " 'protect',\n",
       " 'more',\n",
       " 'people',\n",
       " 'But',\n",
       " 'with',\n",
       " 'more',\n",
       " 'than',\n",
       " '1',\n",
       " 'million',\n",
       " 'deaths',\n",
       " 'from',\n",
       " 'COVID',\n",
       " '19',\n",
       " 'worldwide',\n",
       " 'and',\n",
       " 'U',\n",
       " 'S',\n",
       " 'deaths',\n",
       " 'surpassing',\n",
       " '200',\n",
       " '000',\n",
       " 'the',\n",
       " 'urgency',\n",
       " 'in',\n",
       " 'finding',\n",
       " 'a',\n",
       " 'vaccine',\n",
       " 'that',\n",
       " 'safely',\n",
       " 'helps',\n",
       " 'at',\n",
       " 'least',\n",
       " 'some',\n",
       " 'people',\n",
       " 'is',\n",
       " 'at',\n",
       " 'the',\n",
       " 'forefront']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.1.1. Simply split the text by one or more non-word characters\n",
    "\n",
    "# \\W+: one or more non-words\n",
    "tokens = re.split(r\"\\W+\", text)   \n",
    "\n",
    "# get the number of tokens\n",
    "\n",
    "print(len(tokens))                   \n",
    "print(tokens)                     \n",
    "\n",
    "# Pros: no punctuation, just words\n",
    "# Cons: COVID-19, doesn't, couldn't, 200,000\n",
    "# are split into two words\n",
    "\n",
    "re.findall(r\"\\w+\", text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's word tokenizer does the following steps:\n",
    "* split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n",
    "* treat most punctuation characters as separate tokens\n",
    "* split off commas and single quotes, when followed by whitespace\n",
    "* separate periods that appear at the end of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "['The', 'FDA', 'setting', 'a', 'minimum', 'recommendation', 'for', 'efficacy', 'does', \"n't\", 'mean', 'vaccines', 'could', \"n't\", 'perform', 'better', '.', 'The', 'benchmark', 'is', 'also', 'a', 'reminder', 'that', 'COVID-19', 'vaccine', 'development', 'is', 'in', 'its', 'early', 'days', '.', 'If', 'the', 'first', 'vaccines', 'made', 'available', 'only', 'meet', 'the', 'minimum', ',', 'they', 'may', 'be', 'replaced', 'by', 'others', 'that', 'prove', 'to', 'protect', 'more', 'people', '.', 'But', 'with', 'more', 'than', '1', 'million', 'deaths', 'from', 'COVID-19', 'worldwide', '—', 'and', 'U.S.', 'deaths', 'surpassing', '200,000', '—', 'the', 'urgency', 'in', 'finding', 'a', 'vaccine', 'that', 'safely', 'helps', 'at', 'least', 'some', 'people', 'is', 'at', 'the', 'forefront', '.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.2 NLTK's word tokenizer: \n",
    "\n",
    "# break down text into words and punctuations\n",
    "\n",
    "# invoke NLTK's word tokenizer\n",
    "tokens = nltk.word_tokenize(text)    \n",
    "print(len(tokens) )                   \n",
    "print (tokens)       \n",
    "\n",
    "# Pros: words are well tokenized, \n",
    "# e.g. COVID-19, 200,000 are not split by punctuations\n",
    "# doesn't becomes does n't\n",
    "# cons: need to remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bk/66r4ld3j7hj8yg_49fhv2fr40000gn/T/ipykernel_43295/2585661728.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'—'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# remove empty tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.3 remove leading or trailing punctuations\n",
    "\n",
    "import string\n",
    "\n",
    "string.punctuation\n",
    "\n",
    "tokens=[token.strip(string.punctuation+'—') for token in tokens]\n",
    "tokens\n",
    "# remove empty tokens\n",
    "tokens=[token.strip() for token in tokens \\\n",
    "        if token.strip()!='']\n",
    "print(len(tokens) )\n",
    "print(tokens)  \n",
    "\n",
    "# Note '—' is still kept since it's not in the punctuation list. How to remove it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's regular expression tokinizer (customizable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "['The', 'FDA', 'setting', 'minimum', 'recommendation', 'for', 'efficacy', \"doesn't\", 'mean', 'vaccines', \"couldn't\", 'perform', 'better', 'The', 'benchmark', 'is', 'also', 'reminder', 'that', 'COVID-19', 'vaccine', 'development', 'is', 'in', 'its', 'early', 'days', 'If', 'the', 'first', 'vaccines', 'made', 'available', 'only', 'meet', 'the', 'minimum', 'they', 'may', 'be', 'replaced', 'by', 'others', 'that', 'prove', 'to', 'protect', 'more', 'people', 'But', 'with', 'more', 'than', 'million', 'deaths', 'from', 'COVID-19', 'worldwide', 'and', 'deaths', 'surpassing', '200,000', 'the', 'urgency', 'in', 'finding', 'vaccine', 'that', 'safely', 'helps', 'at', 'least', 'some', 'people', 'is', 'at', 'the', 'forefront']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.4 NLTK's regular expression tokenizer \n",
    "\n",
    "# Pattern can be customized to your need\n",
    "\n",
    "# a word is defined as:\n",
    "# (1) must start with a word character  \\w\n",
    "# (2) then contain zero or more word characters,\"-\",\",\", \n",
    "#     or \"'\" in the middle [\\w\\,'-]*\n",
    "#     e.g.: couldn't, 600,000, COVID-19\n",
    "# (3) must end with a word character \\w\n",
    "\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "\n",
    "# call NLTK's regular expression tokenization\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print(len(tokens))\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise use regular expression tokenizer to extract\n",
    "# course and title pharse, i.e \n",
    "# 'COM-101 COMPUTERS'\n",
    "\n",
    "text = '''COM-101   COMPUTERS\n",
    "COM-111   DATABASE\n",
    "COM-211   ALGORITHM\n",
    "MAT-103   STATISTICS learning\n",
    "MAT-102   STATISTICS'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[\"The FDA setting a minimum recommendation for efficacy doesn't mean vaccines couldn't perform better.\",\n",
       " 'The benchmark is also a reminder that COVID-19 vaccine development is in its early days.',\n",
       " 'If the first vaccines made available only meet the minimum, they may be replaced by others that prove to protect more people.',\n",
       " 'But with more than 1 million deaths from COVID-19 worldwide — and U.S. deaths surpassing 200,000 — the urgency in finding a vaccine that safely helps at least some people is at the forefront.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.2.1. Segmentation by Sentences\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "len(sentences)\n",
    "sentences\n",
    "\n",
    "# what patterns can be used to segment \n",
    "# text into sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Phrases: Bigrams (2 consecutive words),  Trigrams (3 consecutive words), or in general n-grams\n",
    " - Why bigrams and trigrams?\n",
    " - How to get bigrams or trigrams:\n",
    "    1. First tokenize text into unigrams\n",
    "    2. Slice through the list of unigrams to get bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'FDA'), ('FDA', 'setting'), ('setting', 'minimum'), ('minimum', 'recommendation'), ('recommendation', 'for'), ('for', 'efficacy'), ('efficacy', \"doesn't\"), (\"doesn't\", 'mean'), ('mean', 'vaccines'), ('vaccines', \"couldn't\"), (\"couldn't\", 'perform'), ('perform', 'better'), ('better', 'The'), ('The', 'benchmark'), ('benchmark', 'is'), ('is', 'also'), ('also', 'reminder'), ('reminder', 'that'), ('that', 'COVID-19'), ('COVID-19', 'vaccine'), ('vaccine', 'development'), ('development', 'is'), ('is', 'in'), ('in', 'its'), ('its', 'early'), ('early', 'days'), ('days', 'If'), ('If', 'the'), ('the', 'first'), ('first', 'vaccines'), ('vaccines', 'made'), ('made', 'available'), ('available', 'only'), ('only', 'meet'), ('meet', 'the'), ('the', 'minimum'), ('minimum', 'they'), ('they', 'may'), ('may', 'be'), ('be', 'replaced'), ('replaced', 'by'), ('by', 'others'), ('others', 'that'), ('that', 'prove'), ('prove', 'to'), ('to', 'protect'), ('protect', 'more'), ('more', 'people'), ('people', 'But'), ('But', 'with'), ('with', 'more'), ('more', 'than'), ('than', 'million'), ('million', 'deaths'), ('deaths', 'from'), ('from', 'COVID-19'), ('COVID-19', 'worldwide'), ('worldwide', 'and'), ('and', 'deaths'), ('deaths', 'surpassing'), ('surpassing', '200,000'), ('200,000', 'the'), ('the', 'urgency'), ('urgency', 'in'), ('in', 'finding'), ('finding', 'vaccine'), ('vaccine', 'that'), ('that', 'safely'), ('safely', 'helps'), ('helps', 'at'), ('at', 'least'), ('least', 'some'), ('some', 'people'), ('people', 'is'), ('is', 'at'), ('at', 'the'), ('the', 'forefront')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'FDA', 'setting'),\n",
       " ('FDA', 'setting', 'minimum'),\n",
       " ('setting', 'minimum', 'recommendation'),\n",
       " ('minimum', 'recommendation', 'for'),\n",
       " ('recommendation', 'for', 'efficacy'),\n",
       " ('for', 'efficacy', \"doesn't\"),\n",
       " ('efficacy', \"doesn't\", 'mean'),\n",
       " (\"doesn't\", 'mean', 'vaccines'),\n",
       " ('mean', 'vaccines', \"couldn't\"),\n",
       " ('vaccines', \"couldn't\", 'perform'),\n",
       " (\"couldn't\", 'perform', 'better'),\n",
       " ('perform', 'better', 'The'),\n",
       " ('better', 'The', 'benchmark'),\n",
       " ('The', 'benchmark', 'is'),\n",
       " ('benchmark', 'is', 'also'),\n",
       " ('is', 'also', 'reminder'),\n",
       " ('also', 'reminder', 'that'),\n",
       " ('reminder', 'that', 'COVID-19'),\n",
       " ('that', 'COVID-19', 'vaccine'),\n",
       " ('COVID-19', 'vaccine', 'development'),\n",
       " ('vaccine', 'development', 'is'),\n",
       " ('development', 'is', 'in'),\n",
       " ('is', 'in', 'its'),\n",
       " ('in', 'its', 'early'),\n",
       " ('its', 'early', 'days'),\n",
       " ('early', 'days', 'If'),\n",
       " ('days', 'If', 'the'),\n",
       " ('If', 'the', 'first'),\n",
       " ('the', 'first', 'vaccines'),\n",
       " ('first', 'vaccines', 'made'),\n",
       " ('vaccines', 'made', 'available'),\n",
       " ('made', 'available', 'only'),\n",
       " ('available', 'only', 'meet'),\n",
       " ('only', 'meet', 'the'),\n",
       " ('meet', 'the', 'minimum'),\n",
       " ('the', 'minimum', 'they'),\n",
       " ('minimum', 'they', 'may'),\n",
       " ('they', 'may', 'be'),\n",
       " ('may', 'be', 'replaced'),\n",
       " ('be', 'replaced', 'by'),\n",
       " ('replaced', 'by', 'others'),\n",
       " ('by', 'others', 'that'),\n",
       " ('others', 'that', 'prove'),\n",
       " ('that', 'prove', 'to'),\n",
       " ('prove', 'to', 'protect'),\n",
       " ('to', 'protect', 'more'),\n",
       " ('protect', 'more', 'people'),\n",
       " ('more', 'people', 'But'),\n",
       " ('people', 'But', 'with'),\n",
       " ('But', 'with', 'more'),\n",
       " ('with', 'more', 'than'),\n",
       " ('more', 'than', 'million'),\n",
       " ('than', 'million', 'deaths'),\n",
       " ('million', 'deaths', 'from'),\n",
       " ('deaths', 'from', 'COVID-19'),\n",
       " ('from', 'COVID-19', 'worldwide'),\n",
       " ('COVID-19', 'worldwide', 'and'),\n",
       " ('worldwide', 'and', 'deaths'),\n",
       " ('and', 'deaths', 'surpassing'),\n",
       " ('deaths', 'surpassing', '200,000'),\n",
       " ('surpassing', '200,000', 'the'),\n",
       " ('200,000', 'the', 'urgency'),\n",
       " ('the', 'urgency', 'in'),\n",
       " ('urgency', 'in', 'finding'),\n",
       " ('in', 'finding', 'vaccine'),\n",
       " ('finding', 'vaccine', 'that'),\n",
       " ('vaccine', 'that', 'safely'),\n",
       " ('that', 'safely', 'helps'),\n",
       " ('safely', 'helps', 'at'),\n",
       " ('helps', 'at', 'least'),\n",
       " ('at', 'least', 'some'),\n",
       " ('least', 'some', 'people'),\n",
       " ('some', 'people', 'is'),\n",
       " ('people', 'is', 'at'),\n",
       " ('is', 'at', 'the'),\n",
       " ('at', 'the', 'forefront')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.3.1. Get bigrams from the text                       \n",
    "\n",
    "# bigrams are formed from unigrams\n",
    "# nltk.bigram returns an iterator\n",
    "\n",
    "bigrams=list(nltk.bigrams(tokens))  # tokens are created in Exercise 3.1.4\n",
    "print(bigrams)\n",
    "\n",
    "# trigrams\n",
    "list(nltk.trigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Collocation\n",
    " - Most bigrams or trigrams may sound odd. However, we need to pay attention to frequent bigrams or trigrams\n",
    " - **Collocation**: an expression consisting of two or more words that correspond to some conventional way of saying things, e.g. red wine, United States, balance sheet etc.\n",
    "    - Collocations are not fully compositional in that there is usually an element of meaning added to the combination.\n",
    " - Question: how to find collocations?\n",
    "    - Suppose you have a rich collection of text, e.g. english-web.txt\n",
    "    - How to find good collocations from this file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow-Citizens of the Senate and of the House of Representatives:\n",
      "\n",
      "Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was\n"
     ]
    }
   ],
   "source": [
    "# Sample text: inaugural address\n",
    "\n",
    "# To check the text, use\n",
    "\n",
    "print(nltk.corpus.inaugural.raw('1789-Washington.txt')[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 'the'),\n",
       " (',', 'and'),\n",
       " ('in', 'the'),\n",
       " ('to', 'the'),\n",
       " ('of', 'our'),\n",
       " ('.', 'The'),\n",
       " ('.', 'We'),\n",
       " ('and', 'the'),\n",
       " (',', 'the'),\n",
       " ('.', 'It')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.1.\n",
    "# construct bigrams using words from a large bulit-in NLTK corpus\n",
    "\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "# bigram association measures\n",
    "# different measures, e.g. frequency, are implemented\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "# First load text from a NLTK corpus (inagural) \n",
    "# and create unigram tokens\n",
    "# Then create bigrams from the tokens\n",
    "words=nltk.corpus.inaugural.words()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# find the top 10 bigrams by frequency\n",
    "finder.nbest(bigram_measures.raw_freq, 10) \n",
    "\n",
    "# Note that the most frequent bigrams are very odd\n",
    "# how to fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('United', 'States'),\n",
       " ('fellow', 'citizens'),\n",
       " ('let', 'us'),\n",
       " ('Let', 'us'),\n",
       " ('American', 'people'),\n",
       " ('Federal', 'Government'),\n",
       " ('years', 'ago'),\n",
       " ('four', 'years'),\n",
       " ('General', 'Government'),\n",
       " ('upon', 'us'),\n",
       " ('one', 'another'),\n",
       " ('fellow', 'Americans'),\n",
       " ('Vice', 'President'),\n",
       " ('God', 'bless'),\n",
       " ('every', 'citizen'),\n",
       " ('Fellow', 'citizens'),\n",
       " ('Almighty', 'God'),\n",
       " ('foreign', 'nations'),\n",
       " ('Chief', 'Justice'),\n",
       " ('every', 'American')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.2. Find collocation by filter\n",
    "\n",
    "import string\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "#print(stop_words)\n",
    "\n",
    "finder.apply_word_filter(lambda w: w.lower() in stop_words\\\n",
    "                         or w.strip(string.punctuation)=='')\n",
    "\n",
    "finder.nbest(bigram_measures.raw_freq, 20) \n",
    "\n",
    "# better?\n",
    "# notice \"let us\", \"upon us\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 How to find collocations - PMI\n",
    "- By **frequency** (perhaps with filter)\n",
    "- **Pointwise Mutual Information (PMI)**\n",
    "  - giving two words $w_1, w_2$, $$PMI(w_1,w_2)=\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}$$\n",
    "  - Some observations:\n",
    "    - if $w_1$ and $w_2$ are independent, $PMI(w_1,w_2)=0$\n",
    "    - if $w_1$ is completely dependent on $w_2$, i.e. $p(w_1,w_2)=p(w_2)$, $PMI(w_1,w_2)=\\log\\frac{1}{p(w_1)}$. In this case, what if $w_1$ just appears once in the corpus? \n",
    "    - PMI favors less frequent collocations \n",
    "    - how to fix it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/', '11'),\n",
       " ('25', 'straight'),\n",
       " ('Amelia', 'Island'),\n",
       " ('Apollo', 'astronauts'),\n",
       " ('Archibald', 'MacLeish'),\n",
       " ('BUSINESS', 'COOPERATION'),\n",
       " ('Barbary', 'Powers'),\n",
       " ('Belleau', 'Wood'),\n",
       " ('Boston', 'lawyer'),\n",
       " ('Britannic', 'Majesty'),\n",
       " ('COOPERATION', 'BY'),\n",
       " ('CRIMINAL', 'JUSTICE'),\n",
       " ('Calvin', 'Coolidge'),\n",
       " ('Cape', 'Horn'),\n",
       " ('Cardinal', 'Bernardin'),\n",
       " ('Chop', 'Hill'),\n",
       " ('Chosin', 'Reservoir'),\n",
       " ('Christmas', 'Eve'),\n",
       " ('Colonel', 'Goethals'),\n",
       " ('Dark', 'pictures')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.4.1.1 Metrics for Collocations\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# construct bigrams using words from a NLTK corpus\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# find top-n bigrams by pmi\n",
    "finder.nbest(bigram_measures.pmi, 20) \n",
    "\n",
    "# Notice most of them are names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Indian', 'tribes'),\n",
       " ('Western', 'Hemisphere'),\n",
       " ('¡', 'Xand'),\n",
       " ('coordinate', 'branches'),\n",
       " ('Old', 'World'),\n",
       " ('George', 'Washington'),\n",
       " ('faithfully', 'executed'),\n",
       " ('nuclear', 'weapons'),\n",
       " ('Chief', 'Magistrate'),\n",
       " ('middle', 'class'),\n",
       " ('Chief', 'Justice'),\n",
       " ('tariff', 'bill'),\n",
       " ('World', 'War'),\n",
       " ('executive', 'department'),\n",
       " ('move', 'forward'),\n",
       " ('President', 'Bush'),\n",
       " ('Vice', 'President'),\n",
       " ('executive', 'branch'),\n",
       " ('interstate', 'commerce'),\n",
       " ('domestic', 'concerns')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4.1.2 filter bigrams by frequency\n",
    "\n",
    "finder.apply_freq_filter(5)  #5\n",
    "finder.nbest(bigram_measures.pmi, 20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 How to find collocations - NPMI and others\n",
    "- **Normalized Pointwise Mutual Information (`NPMI`)**\n",
    "   - If $w_1$ and $w_2$ always occur together, i.e., $p(w_1)=p(w_2)=p(w_1,w_2)$, PMI reaches the maximum: $$PMI(w_1,w_2)=-\\log{p(w_1)}=-\\log{p(w_2)}=-\\log{p(w_1,w_2)}$$\n",
    "   - Normalized PMI is the PMI divided by the upper bound:\n",
    "   $$NPMI(w_1,w_2)=\\frac{\\log{\\frac{p(w_1,w_2)}{p(w_1)*p(w_2)}}}{-\\log{p(w_1,w_2)}}$$\n",
    "   \n",
    "- Another simple method by Mikolov et al. (2013) (https://arxiv.org/pdf/1310.4546.pdf):\n",
    "\n",
    "    - $Score(w_1, w_2)=\\frac{count(w_1,w_2)-\\delta}{count(w_1)*count(w_2)}, \\text{where}~\\delta~\\text{is the minimum collocation frequency} $ \n",
    "\n",
    "    - This is equivalent to PMI with a minimum collocation threshold\n",
    "- Both methods are implemented in `gensim` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 Phrase extraction by Gensim package\n",
    "- Gensim is an open source Python library for NLP, with a focus on topic modeling.\n",
    "- It is not an everything-including-the-kitchen-sink NLP research library (like NLTK); instead, Gensim is a mature, focused, and efficient suite of NLP tools for topic modeling, including \n",
    "  - Word2Vec word embedding \n",
    "  - Topic modeling\n",
    "  - Text preprocessing like **phrase extraction**\n",
    "  \n",
    "- Gensim Phrase Model: \n",
    "    - `gensim.models.phrases.Phrases(sentences, min_count, threshold, max_vocab_size, delimiter, scoring, ...)`\n",
    "        - `sentences`: list of sentences or iterables, each of which can be a document\n",
    "        - `min_count`: Ignore all words and bigrams with total collected count lower than this value.\n",
    "        - `threshold`: Represent a score threshold for forming the phrases (higher means fewer phrases). A phrase of words $a$ followed by $b$ is accepted if the score of the phrase is greater than threshold. Heavily depends on concrete scoring-function.\n",
    "        - `max_vocab_size`: Maximum size (number of tokens) of the vocabulary. \n",
    "        - `delimiter`: Glue character used to join collocation tokens, should be a byte string (e.g. '\\_').\n",
    "        - `scoring`: Specify how potential phrases are scored. \n",
    "           - `default` - original_scorer(), by Mikolov et al. (2013) (https://arxiv.org/pdf/1310.4546.pdf)\n",
    "           - `npmi` - npmi_scorer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santo_Domingo:\t8549.11\n",
      "Indian_tribes:\t6411.83\n",
      "Abraham_Lincoln:\t6411.83\n",
      "Founding_Fathers:\t6411.83\n",
      "Social_Security:\t6411.83\n",
      "specie_payments:\t6411.83\n",
      "illegal_liquor:\t5129.47\n",
      "merchant_marine:\t4808.88\n",
      "Western_Hemisphere:\t4808.88\n",
      "founding_documents:\t4274.56\n",
      "Supreme_Court:\t4274.56\n",
      "lock_type:\t3847.10\n",
      "Old_World:\t3108.77\n",
      "inland_frontiers:\t2747.93\n",
      "¡_Xand:\t2564.73\n",
      "coordinate_branches:\t2355.37\n",
      "Thomas_Jefferson:\t2331.58\n",
      "eighteenth_amendment:\t2137.28\n",
      "Chief_Magistrate:\t1998.49\n",
      "extra_session:\t1972.87\n",
      "Great_Britain:\t1923.55\n",
      "George_Washington:\t1846.61\n",
      "silent_prayer:\t1810.40\n",
      "faithfully_executed:\t1803.33\n",
      "entangling_alliances:\t1748.68\n",
      "fervent_supplications:\t1709.82\n",
      "nuclear_weapons:\t1648.76\n",
      "distinguished_guests:\t1538.84\n",
      "Civil_War:\t1465.56\n",
      "onward_march:\t1424.85\n",
      "plainly_written:\t1373.96\n",
      "Chief_Justice:\t1373.96\n",
      "middle_class:\t1221.30\n",
      "fifteenth_amendment:\t1068.64\n",
      "earliest_practicable:\t961.78\n",
      "fertile_soil:\t961.78\n",
      "preceding_term:\t961.78\n",
      "walk_humbly:\t874.34\n",
      "World_War:\t814.20\n",
      "tariff_bill:\t744.60\n",
      "regular_session:\t739.83\n",
      "Vice_President:\t682.03\n",
      "executive_department:\t646.57\n",
      "standing_armies:\t635.88\n",
      "technical_knowledge:\t625.54\n",
      "hard_choices:\t610.65\n",
      "Divine_Providence:\t601.11\n",
      "move_forward:\t601.11\n",
      "President_Bush:\t539.94\n",
      "entirely_consistent:\t538.06\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.1. Find bigrams using gensim\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "\n",
    "words=nltk.corpus.inaugural.words()\n",
    "\n",
    "# Train phrase model to find phrases using scorer (Mikolov et al. 2013)\n",
    "phrases = Phrases([words], min_count=2, threshold=50)\n",
    "\n",
    "# get unique set of phrases and sorted by score in descending order\n",
    "items = sorted(set(phrases.find_phrases([words]).items()), key=lambda item: -item[1])\n",
    "\n",
    "# print top 50 phrases\n",
    "for phrase, score in items[0:50]:\n",
    "    print(\"{0}:\\t{1:.2f}\".format(phrase, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santo_Domingo:\t1.00\n",
      "Philippine_Islands:\t1.00\n",
      "reverend_clergy:\t1.00\n",
      "Information_Age:\t1.00\n",
      "Rocky_Mountains:\t1.00\n",
      "Porto_Rico:\t1.00\n",
      "Panama_Canal:\t1.00\n",
      "Social_Security:\t0.97\n",
      "Founding_Fathers:\t0.97\n",
      "Indian_tribes:\t0.97\n",
      "'_s:\t0.96\n",
      "specie_payments:\t0.96\n",
      "Abraham_Lincoln:\t0.96\n",
      "illegal_liquor:\t0.95\n",
      "merchant_marine:\t0.95\n",
      "Majority_Leader:\t0.94\n",
      "electors_residing:\t0.94\n",
      "Western_Hemisphere:\t0.94\n",
      "founding_documents:\t0.94\n",
      "Old_World:\t0.93\n",
      "sheet_anchor:\t0.93\n",
      "lock_type:\t0.93\n",
      "Supreme_Court:\t0.92\n",
      "Dingley_Act:\t0.92\n",
      "secondary_boycott:\t0.92\n",
      "Middle_East:\t0.92\n",
      "cleaner_environment:\t0.92\n",
      "start_afresh:\t0.92\n",
      "Permanent_Court:\t0.90\n",
      "elective_franchise:\t0.90\n",
      "inland_frontiers:\t0.90\n",
      "Chief_Magistrate:\t0.88\n",
      "United_States:\t0.88\n",
      "200th_anniversary:\t0.88\n",
      "Thomas_Jefferson:\t0.88\n",
      "¡_Xand:\t0.88\n",
      "coordinate_branches:\t0.87\n",
      "Chief_Justice:\t0.87\n",
      "exclusive_metallic:\t0.87\n",
      "Pacific_Coast:\t0.87\n",
      "extra_session:\t0.86\n",
      "eighteenth_amendment:\t0.86\n",
      "Senator_Mathias:\t0.86\n",
      "Senator_Dole:\t0.86\n",
      "temporary_restraining:\t0.86\n",
      "entangling_alliances:\t0.85\n",
      "fugitive_slaves:\t0.85\n",
      "fervent_supplications:\t0.85\n",
      "Great_Britain:\t0.85\n",
      "George_Washington:\t0.84\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.2. Find bigrams by NPMI\n",
    "\n",
    "# find phrases using NPMI\n",
    "\n",
    "phrases = Phrases([words], min_count=2, threshold=0.5, \\\n",
    "                  scoring='npmi')\n",
    "\n",
    "# get unique set of phrases and sorted by score in descending order\n",
    "items = sorted(set(phrases.find_phrases([words]).items()), key=lambda item: -item[1])\n",
    "\n",
    "# print top 20 phrases\n",
    "for phrase, score in items[0:50]:\n",
    "    print(\"{0}:\\t{1:.2f}\".format(phrase, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
      "['That', 'we', 'are', 'in', 'the', 'midst', 'of', 'crisis', 'is', 'now', 'well', 'understood.', 'Our', 'nation', 'is', 'at', 'war,', 'against', 'a', 'far-reaching', 'network', 'of', 'violence', 'and', 'hatred.', 'Our', 'economy', 'is', 'badly', 'weakened,', 'a', 'consequence', 'of', 'greed', 'and', 'irresponsibility', 'on', 'the', 'part', 'of', 'some,', 'but', 'also', 'our', 'collective', 'failure', 'to', 'make', 'hard_choices', 'and', 'prepare', 'the', 'nation', 'for', 'a', 'new', 'age.', 'Homes', 'have_been', 'lost;', 'jobs', 'shed;', 'businesses', 'shuttered.', 'Our', 'health_care', 'is', 'too', 'costly;', 'our', 'schools', 'fail', 'too', 'many;', 'and', 'each', 'day', 'brings', 'further', 'evidence', 'that', 'the', 'ways', 'we', 'use', 'energy', 'strengthen', 'our', 'adversaries', 'and', 'threaten', 'our', 'planet.']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2.3. Tokenize by unigrams and bigrams\n",
    "\n",
    "# Initialize phrase tokenizer\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "\n",
    "#sent = nltk.corpus.inaugural.raw('2009-Obama.txt')\n",
    "\n",
    "sent = '''That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.'''\n",
    "\n",
    "print(sent)\n",
    "\n",
    "print(bigram[sent.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary \n",
    " - Vocabulary: the set of unique tokens (unigrams/phrases)  \n",
    " - Dictionary: typicallly, the vocabulary of a text can be represented as a dictionary \n",
    "    * Key: word, Value: count of the word\n",
    "    * **nltk.FreqDist()**: a nice function for calculating frequncy of words/phrases\n",
    "        - Get the frequency of items in the parameter list \n",
    "        - Retruns an object similar to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 6, 'is': 3, 'that': 3, 'minimum': 2, 'vaccines': 2, 'covid-19': 2, 'vaccine': 2, 'in': 2, 'more': 2, 'people': 2, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 words: [('the', 6), ('is', 3), ('that', 3), ('minimum', 2), ('vaccines', 2)]\n",
      "the : 6\n",
      "is : 3\n",
      "that : 3\n",
      "minimum : 2\n",
      "vaccines : 2\n",
      "covid-19 : 2\n",
      "vaccine : 2\n",
      "in : 2\n",
      "more : 2\n",
      "people : 2\n",
      "deaths : 2\n",
      "at : 2\n",
      "fda : 1\n",
      "setting : 1\n",
      "recommendation : 1\n",
      "for : 1\n",
      "efficacy : 1\n",
      "doesn't : 1\n",
      "mean : 1\n",
      "couldn't : 1\n",
      "perform : 1\n",
      "better : 1\n",
      "benchmark : 1\n",
      "also : 1\n",
      "reminder : 1\n",
      "development : 1\n",
      "its : 1\n",
      "early : 1\n",
      "days : 1\n",
      "if : 1\n",
      "first : 1\n",
      "made : 1\n",
      "available : 1\n",
      "only : 1\n",
      "meet : 1\n",
      "they : 1\n",
      "may : 1\n",
      "be : 1\n",
      "replaced : 1\n",
      "by : 1\n",
      "others : 1\n",
      "prove : 1\n",
      "to : 1\n",
      "protect : 1\n",
      "but : 1\n",
      "with : 1\n",
      "than : 1\n",
      "million : 1\n",
      "from : 1\n",
      "worldwide : 1\n",
      "and : 1\n",
      "surpassing : 1\n",
      "200,000 : 1\n",
      "urgency : 1\n",
      "finding : 1\n",
      "safely : 1\n",
      "helps : 1\n",
      "least : 1\n",
      "some : 1\n",
      "forefront : 1\n"
     ]
    }
   ],
   "source": [
    "# 3.5.1 Get token frequency\n",
    "\n",
    "# first tokenize the text\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "#tokens\n",
    "# get unigram frequency \n",
    "# recall, you can also get the dictionary by \n",
    "# {token:count(token) for token in set(tokens)}\n",
    "\n",
    "word_dist=nltk.FreqDist(tokens)\n",
    "word_dist\n",
    "\n",
    "# get the most frequent items\n",
    "print(\"top 5 words:\", word_dist.most_common(5))\n",
    "\n",
    "# what kind of words usually have high frequency?\n",
    "\n",
    "# it behaves as a dictionary\n",
    "for word in word_dist:\n",
    "    print(word,\":\", word_dist[word])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Stop words and word filtering\n",
    "\n",
    " - Stop words: a set of commonly used words, have very little meaning, and cannot differentiate a text from others, such as \"and\", \"the\" etc. \n",
    " - Stop words are typically ignored in NLP processing or by search engine\n",
    " - Stop words usually are application specific. You can define your own stop words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum': 2,\n",
       " 'vaccines': 2,\n",
       " 'vaccine': 2,\n",
       " 'people': 2,\n",
       " 'deaths': 2,\n",
       " 'fda': 1,\n",
       " 'setting': 1,\n",
       " 'recommendation': 1,\n",
       " 'efficacy': 1,\n",
       " 'mean': 1,\n",
       " 'perform': 1,\n",
       " 'better': 1,\n",
       " 'benchmark': 1,\n",
       " 'also': 1,\n",
       " 'reminder': 1,\n",
       " 'development': 1,\n",
       " 'early': 1,\n",
       " 'days': 1,\n",
       " 'first': 1,\n",
       " 'made': 1,\n",
       " 'available': 1,\n",
       " 'meet': 1,\n",
       " 'may': 1,\n",
       " 'replaced': 1,\n",
       " 'others': 1,\n",
       " 'prove': 1,\n",
       " 'protect': 1,\n",
       " 'million': 1,\n",
       " 'worldwide': 1,\n",
       " 'surpassing': 1,\n",
       " '200,000': 1,\n",
       " 'urgency': 1,\n",
       " 'finding': 1,\n",
       " 'safely': 1,\n",
       " 'helps': 1,\n",
       " 'least': 1,\n",
       " 'forefront': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('minimum', 2),\n",
       " ('vaccines', 2),\n",
       " ('vaccine', 2),\n",
       " ('people', 2),\n",
       " ('deaths', 2),\n",
       " ('fda', 1),\n",
       " ('setting', 1),\n",
       " ('recommendation', 1),\n",
       " ('efficacy', 1),\n",
       " ('mean', 1),\n",
       " ('perform', 1),\n",
       " ('better', 1),\n",
       " ('benchmark', 1),\n",
       " ('also', 1),\n",
       " ('reminder', 1),\n",
       " ('development', 1),\n",
       " ('early', 1),\n",
       " ('days', 1),\n",
       " ('first', 1),\n",
       " ('made', 1),\n",
       " ('available', 1),\n",
       " ('meet', 1),\n",
       " ('may', 1),\n",
       " ('replaced', 1),\n",
       " ('others', 1),\n",
       " ('prove', 1),\n",
       " ('protect', 1),\n",
       " ('million', 1),\n",
       " ('worldwide', 1),\n",
       " ('surpassing', 1),\n",
       " ('200,000', 1),\n",
       " ('urgency', 1),\n",
       " ('finding', 1),\n",
       " ('safely', 1),\n",
       " ('helps', 1),\n",
       " ('least', 1),\n",
       " ('forefront', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.5.1.1\n",
    "# get NLTK English stop words\n",
    "# You can modify this list by adding more stop words or remove stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words+=[\"covid-19\", \"virus\"]\n",
    "#print (stop_words)\n",
    "\n",
    "# filter stop words out of the dictionary\n",
    "# by creating a new dictionary\n",
    "\n",
    "filtered_dict={word: word_dist[word] \\\n",
    "                     for word in word_dist \\\n",
    "                     if word not in stop_words}\n",
    "\n",
    "\n",
    "filtered_dict\n",
    "\n",
    "# how to sort the dictionary by value?\n",
    "sorted(filtered_dict.items(), key = lambda item: -item[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 positive/negative words: sentiment analysis\n",
    "- Sentiment analysis often relies on **lists of words and phrases with positive and negative connotations**. \n",
    "- Many dictionaries of positive and negative opinion words were already developed:\n",
    "\n",
    "  - **Hu and Liu's lexicon**: http://www.cs.uic.edu/~liub/FBS/\n",
    "  - **SentiWordNet**: an excellent publicly available lexicon (http://sentiwordnet.isti.cnr.it/) \n",
    "  - **SentiWords**: contains 155,000 English words (https://hlt-nlp.fbk.eu/technologies/sentiwords)\n",
    "  - **WordStat**: contains more than 9164 negative and 4847 positive word patterns (https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/sentiment-dictionaries/)\n",
    "  - **SenticNet**: provides polarity associated with 50,000 natural language concepts https://sentic.net\n",
    "  - **Sentiment140**:  created from 1.6 million tweets and contains a list of words and their associations with positive and negative sentiment (https://github.com/felipebravom/StaticTwitterSent/tree/master/extra/Sentiment140-Lexicon-v0.1)\n",
    "- Opinion words are <b>domain-specific</b>. (e.g. \"power\" in political domain vs. in engergy sector)\n",
    "  - For example, for financial industry, there are a number of dictionaries for opinion words:\n",
    "     * Harvard's General Inquirer (GI): http://www.wjh.harvard.edu/~inquirer/\n",
    "     * Loughran and McDonald (2015):  https://sraf.nd.edu/textual-analysis/resources/\n",
    "- For description of these lexicons, check https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c\n",
    "- Question: **How to select the right lexicon**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ambitious', 'thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5.2.1\n",
    "# Find positive words \n",
    "text = '''the problem is that the writers, james cameron and jay cocks ,\\\n",
    "were too ambitious, aiming for a film with social relevance, thrills, and drama. \n",
    " not that ambitious film-making should be discouraged; \\\n",
    " just that when it fails to achieve its goals, it fails badly and obviously. \n",
    " the film just ends up preachy, unexciting and uninvolving.'''\n",
    "\n",
    "pattern=r'\\w[\\w\\',-]*\\w'                        \n",
    "tokens=nltk.regexp_tokenize(text.lower(), pattern)\n",
    "\n",
    "\n",
    "with open(\"positive-words.txt\",'r') as f:\n",
    "    positive_words=[line.strip() for line in f]\n",
    "\n",
    "#positive_words\n",
    "#print(positive_words)\n",
    "\n",
    "positive_tokens=[token for token in tokens \\\n",
    "                 if token in positive_words]\n",
    "\n",
    "print(positive_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Naive sentiment analysis**:\n",
    "  - Find positive/negative words\n",
    "  - If more positive words than negative, then positive\n",
    "  - Otherwise, negative\n",
    "- Note the sentence: \n",
    "  -  \"the problem is that the writers, james cameron and jay cocks , were **<font color=\"red\">too ambitious</font>**, aiming for a film with social relevance, thrills, and drama. **<font color=\"red\">not that ambitious</font>** film-making should be discouraged; just that when it fails to achieve its goals ...\"\n",
    "- How to deal with **negation**?\n",
    "- Some useful rules:\n",
    "    - Negative sentiment: \n",
    "      - negative words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - positive words preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "    - Positive sentiment (in the similar fashion):\n",
    "      - positive words not preceded by a negation within $n$ (e.g. three) words in the same sentence.\n",
    "      - negative terms following a negation within  $n$ (e.g. three) words in the same sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thrills', 'ambitious']\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5.2.2 # check if a positive word is preceded by negation words\n",
    "# e.g. not, too, n't, no, cannot\n",
    "\n",
    "# this is not an exhaustive list of negation words!\n",
    "negations=['not', 'too', 'n\\'t', 'no', 'cannot', 'neither','nor', 'little','few']\n",
    "tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "#print(tokens)\n",
    "\n",
    "positive_tokens=[]\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token in positive_words:\n",
    "        if idx>0:\n",
    "            if tokens[idx-1] not in negations:\n",
    "                positive_tokens.append(token)\n",
    "        else:\n",
    "            positive_tokens.append(token)\n",
    "\n",
    "\n",
    "print(positive_tokens)\n",
    "\n",
    "# what if a positive word is preceded \n",
    "# by a negation within N words? \n",
    "# e.g. 'does not make any customer happy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
